{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Exercise 6: Support Vector Machines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io \n",
    "from sklearn import svm \n",
    "import re \n",
    "import pandas as pd\n",
    "from stemming.porter2 import stem\n",
    "import nltk, nltk.stem.porter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Preprocessing Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emailSample1.txt:\n",
      "> Anyone knows how much it costs to host a web portal ?\n",
      ">\n",
      "Well, it depends on how many visitors you're expecting.\n",
      "This can be anywhere from less than 10 bucks a month to a couple of $100. \n",
      "You should checkout http://www.rackspace.com/ or perhaps Amazon EC2 \n",
      "if youre running something big..\n",
      "\n",
      "To unsubscribe yourself from this mailing list, send an email to:\n",
      "groupname-unsubscribe@egroups.com\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "email = ''\n",
    "print(\"emailSample1.txt:\")\n",
    "email_sample = open(\"ex6/emailSample1.txt\")\n",
    "print(email_sample.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess( email ):\n",
    "    email = email.lower()\n",
    "    email = re.sub(\"<[^<>]+>\", \"\", email)\n",
    "    email = re.sub(\"(http|https)://[^\\s]*\", \"httpaddr\", email)\n",
    "    email = re.sub(\"[^\\s]+@[^\\s]+\", 'emailaddr', email)\n",
    "    email = re.sub(\"[\\d]+\", \"number\", email)\n",
    "    email = re.sub(\"[\\$]+\", \"dollar\", email)\n",
    "    return email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folks,\n",
      " \n",
      "my first time posting - have a bit of unix experience, but am new to linux.\n",
      "\n",
      " \n",
      "just got a new pc at home - dell box with windows xp. added a second hard disk\n",
      "for linux. partitioned the disk and have installed suse number.number from cd, which went\n",
      "fine except it didn't pick up my monitor.\n",
      " \n",
      "i have a dell branded enumberfpp number\" lcd flat panel monitor and a nvidia geforcenumber\n",
      "tinumber video card, both of which are probably too new to feature in suse's default\n",
      "set. i downloaded a driver from the nvidia website and installed it using rpm.\n",
      "then i ran saxnumber (as was recommended in some postings i found on the net), but\n",
      "it still doesn't feature my video card in the available list. what next?\n",
      " \n",
      "another problem. i have a dell branded keyboard and if i hit caps-lock twice,\n",
      "the whole machine crashes (in linux, not windows) - even the on/off switch is\n",
      "inactive, leaving me to reach for the power cable instead.\n",
      " \n",
      "if anyone can help me in any way with these probs., i'd be really grateful -\n",
      "i've searched the 'net but have run out of ideas.\n",
      " \n",
      "or should i be going for a different version of linux such as redhat? opinions\n",
      "welcome.\n",
      " \n",
      "thanks a lot,\n",
      "peter\n",
      "\n",
      "-- \n",
      "irish linux users' group: emailaddr\n",
      "httpaddr for (un)subscription information.\n",
      "list maintainer: emailaddr\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"ex6/emailSample2.txt\") as f:\n",
    "    print(preProcess(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on PorterStemmer in module nltk.stem.porter object:\n",
      "\n",
      "class PorterStemmer(nltk.stem.api.StemmerI)\n",
      " |  PorterStemmer(mode='NLTK_EXTENSIONS')\n",
      " |  \n",
      " |  A word stemmer based on the Porter stemming algorithm.\n",
      " |  \n",
      " |      Porter, M. \"An algorithm for suffix stripping.\"\n",
      " |      Program 14.3 (1980): 130-137.\n",
      " |      \n",
      " |  See http://www.tartarus.org/~martin/PorterStemmer/ for the homepage\n",
      " |  of the algorithm.\n",
      " |      \n",
      " |  Martin Porter has endorsed several modifications to the Porter\n",
      " |  algorithm since writing his original paper, and those extensions are\n",
      " |  included in the implementations on his website. Additionally, others\n",
      " |  have proposed further improvements to the algorithm, including NLTK\n",
      " |  contributors. There are thus three modes that can be selected by\n",
      " |  passing the appropriate constant to the class constructor's `mode`\n",
      " |  attribute:\n",
      " |  \n",
      " |      PorterStemmer.ORIGINAL_ALGORITHM\n",
      " |      - Implementation that is faithful to the original paper.\n",
      " |      \n",
      " |        Note that Martin Porter has deprecated this version of the\n",
      " |        algorithm. Martin distributes implementations of the Porter\n",
      " |        Stemmer in many languages, hosted at:\n",
      " |        \n",
      " |          http://www.tartarus.org/~martin/PorterStemmer/\n",
      " |          \n",
      " |        and all of these implementations include his extensions. He\n",
      " |        strongly recommends against using the original, published\n",
      " |        version of the algorithm; only use this mode if you clearly\n",
      " |        understand why you are choosing to do so.\n",
      " |      \n",
      " |      PorterStemmer.MARTIN_EXTENSIONS\n",
      " |      - Implementation that only uses the modifications to the\n",
      " |        algorithm that are included in the implementations on Martin\n",
      " |        Porter's website. He has declared Porter frozen, so the\n",
      " |        behaviour of those implementations should never change.\n",
      " |        \n",
      " |      PorterStemmer.NLTK_EXTENSIONS (default)\n",
      " |      - Implementation that includes further improvements devised by\n",
      " |        NLTK contributors or taken from other modified implementations\n",
      " |        found on the web.\n",
      " |        \n",
      " |  For the best stemming, you should use the default NLTK_EXTENSIONS\n",
      " |  version. However, if you need to get the same results as either the\n",
      " |  original algorithm or one of Martin Porter's hosted versions for\n",
      " |  compability with an existing implementation or dataset, you can use\n",
      " |  one of the other modes instead.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      PorterStemmer\n",
      " |      nltk.stem.api.StemmerI\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, mode='NLTK_EXTENSIONS')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __unicode__ = __str__(self, /)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  stem(self, word)\n",
      " |      Strip affixes from the token and return the stem.\n",
      " |      \n",
      " |      :param token: The token that should be stemmed.\n",
      " |      :type token: str\n",
      " |  \n",
      " |  unicode_repr = __repr__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  MARTIN_EXTENSIONS = 'MARTIN_EXTENSIONS'\n",
      " |  \n",
      " |  NLTK_EXTENSIONS = 'NLTK_EXTENSIONS'\n",
      " |  \n",
      " |  ORIGINAL_ALGORITHM = 'ORIGINAL_ALGORITHM'\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from nltk.stem.api.StemmerI:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.stem.porter.PorterStemmer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email2TokenList( raw_email ):\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    raw_email = preProcess(raw_email)\n",
    "    tokens = re.split(\"[ \\`\\~\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\-\\_\\=\\+\\[\\]\\{\\}\\;\\:\\'\\\"\\|\\\\\\,\\<\\>\\.\\/\\?\\s]\", raw_email)\n",
    "    tokenlist = []\n",
    "    for token in tokens:\n",
    "        token = re.sub(\"^a-zA-Z0-9\", \"\", token)\n",
    "        stemmed = stemmer.stem(token)\n",
    "        if len(token) == 0:\n",
    "            continue\n",
    "        tokenlist.append(stemmed)\n",
    "    return tokenlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_email = open(\"ex6/emailSample1.txt\", 'r')\n",
    "#raw_email = email2TokenList(raw_email.read())\n",
    "raw_email = raw_email.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 Vocabulary List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVocabDict(reverse = False):\n",
    "    vocab_dict = dict()\n",
    "    with open(\"ex6\\\\vocab.txt\") as f:\n",
    "        for line in f:\n",
    "            value, key = line.split()\n",
    "            if not reverse:\n",
    "                vocab_dict[key] = int(value)\n",
    "            else:\n",
    "                vocab_dict[int(value)] = key\n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "531"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict = getVocabDict(False)\n",
    "vocab_dict['emailaddr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email2VocabIndices(raw_email, vocab_dict):\n",
    "    token_list = email2TokenList(raw_email)\n",
    "    vocab_indices = [vocab_dict[token] for token in token_list if token in vocab_dict]\n",
    "    return vocab_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[86,\n",
       " 916,\n",
       " 794,\n",
       " 1077,\n",
       " 883,\n",
       " 370,\n",
       " 1699,\n",
       " 790,\n",
       " 1822,\n",
       " 1831,\n",
       " 883,\n",
       " 431,\n",
       " 1171,\n",
       " 794,\n",
       " 1002,\n",
       " 1893,\n",
       " 1364,\n",
       " 592,\n",
       " 1676,\n",
       " 238,\n",
       " 162,\n",
       " 89,\n",
       " 688,\n",
       " 945,\n",
       " 1663,\n",
       " 1120,\n",
       " 1062,\n",
       " 1699,\n",
       " 375,\n",
       " 1162,\n",
       " 479,\n",
       " 1893,\n",
       " 1510,\n",
       " 799,\n",
       " 1182,\n",
       " 1237,\n",
       " 810,\n",
       " 1895,\n",
       " 1440,\n",
       " 1547,\n",
       " 181,\n",
       " 1699,\n",
       " 1758,\n",
       " 1896,\n",
       " 688,\n",
       " 1676,\n",
       " 992,\n",
       " 961,\n",
       " 1477,\n",
       " 71,\n",
       " 530,\n",
       " 1699,\n",
       " 531]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email2VocabIndices(raw_email, vocab_dict )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Extracting Features from Emails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email2FeatureVector(raw_email, vocab_dict):\n",
    "    n = len(vocab_dict)\n",
    "    vocab_list = np.zeros((n, 1))\n",
    "    vocab_indices = email2VocabIndices(raw_email, vocab_dict)\n",
    "    for vocab in vocab_indices:\n",
    "        #print(vocab)\n",
    "        vocab_list[vocab] = 1\n",
    "    return vocab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1899"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(email2FeatureVector(raw_email, vocab_dict ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of feature vector: 1899\n",
      "Number of 1's entries: 45\n"
     ]
    }
   ],
   "source": [
    "vocab_dict = getVocabDict()\n",
    "email_contents = open(\"ex6\\\\emailSample1.txt\").read()\n",
    "feature_vector = email2FeatureVector(email_contents, vocab_dict)\n",
    "\n",
    "print(\"Length of feature vector: {}\".format(len(feature_vector)))\n",
    "print(\"Number of 1's entries: {}\".format(sum(feature_vector==1)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Training SVM for spam classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = 'ex6/spamTrain.mat'\n",
    "mat = scipy.io.loadmat( datafile )\n",
    "X, y = mat['X'], mat['y']\n",
    "#X =     np.insert(X    ,0,1,axis=1)\n",
    "\n",
    "datafile = 'ex6/spamTest.mat'\n",
    "mat = scipy.io.loadmat( datafile )\n",
    "Xtest, ytest = mat['Xtest'], mat['ytest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training samples: 4000\n",
      "Number of training spam emails: 1277\n",
      "Number of training nonspam emails: 2723\n"
     ]
    }
   ],
   "source": [
    "pos = np.array([X[i] for i in range(X.shape[0]) if y[i] == 1])\n",
    "neg = np.array([X[i] for i in range(X.shape[0]) if y[i] == 0])\n",
    "print(\"Total number of training samples: {}\".format(X.shape[0]))\n",
    "print(\"Number of training spam emails: {}\".format(pos.shape[0]))\n",
    "print(\"Number of training nonspam emails: {}\".format(neg.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_svm = svm.SVC(C=0.1, kernel='linear')\n",
    "\n",
    "linear_svm.fit(X, y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  99.825\n",
      "Test Accuracy:  98.9\n"
     ]
    }
   ],
   "source": [
    "train_prediction = linear_svm.predict(X).reshape((y.shape[0], 1))\n",
    "train_acc = 100 * np.sum(train_prediction==y) / y.shape[0]\n",
    "print(\"Training Accuracy: \", train_acc)\n",
    "\n",
    "test_prediction = linear_svm.predict(Xtest).reshape((ytest.shape[0], 1))\n",
    "test_acc = 100 * np.sum(test_prediction==ytest) / ytest.shape[0]\n",
    "print(\"Test Accuracy: \", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top predictors for Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Most Important\n",
      "0        otherwis\n",
      "1         clearli\n",
      "2           remot\n",
      "3              gt\n",
      "4            visa\n",
      "5            base\n",
      "6           doesn\n",
      "7            wife\n",
      "8        previous\n",
      "9          player\n",
      "10        mortgag\n",
      "11          natur\n",
      "12             ll\n",
      "13          futur\n",
      "14            hot\n",
      "\n",
      "\n",
      "   Least Important\n",
      "0             http\n",
      "1             toll\n",
      "2               xp\n",
      "3            ratio\n",
      "4           august\n",
      "5       unsubscrib\n",
      "6          useless\n",
      "7         numberth\n",
      "8            round\n",
      "9            linux\n",
      "10         datapow\n",
      "11           wrong\n",
      "12          urgent\n",
      "13            that\n",
      "14            spam\n",
      "\n",
      "\n",
      "# of spam containing \"otherwis\" = 804/1277 = 62.96%\n",
      "# of NON spam containing \"otherwis\" = 301/2723 = 11.05%\n"
     ]
    }
   ],
   "source": [
    "vocab_dict_flipped = getVocabDict(reverse=True)\n",
    "\n",
    "sorted_indices = np.argsort( linear_svm.coef_, axis=None )[::-1]\n",
    "\n",
    "most_important_features = pd.DataFrame([ vocab_dict_flipped[x] for x in sorted_indices[:15] ], columns=[\"Most Important\"])\n",
    "\n",
    "least_important_features = pd.DataFrame([ vocab_dict_flipped[x] for x in sorted_indices[-15:] ], columns=[\"Least Important\"])\n",
    "\n",
    "print(most_important_features)\n",
    "print(\"\\n\")\n",
    "print(least_important_features)\n",
    "print(\"\\n\")\n",
    "# Most common word (mostly to debug):\n",
    "most_common_word = vocab_dict_flipped[sorted_indices[0]]\n",
    "print('# of spam containing \\\"%s\\\" = %d/%d = %0.2f%%'% \\\n",
    "    (most_common_word, sum(pos[:,1190]),pos.shape[0],  \\\n",
    "     100.*float(sum(pos[:,1190]))/pos.shape[0]))\n",
    "print('# of NON spam containing \\\"%s\\\" = %d/%d = %0.2f%%'% \\\n",
    "    (most_common_word, sum(neg[:,1190]),neg.shape[0],      \\\n",
    "     100.*float(sum(neg[:,1190]))/neg.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
